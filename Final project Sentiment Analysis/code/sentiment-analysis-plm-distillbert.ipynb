{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10370500,"sourceType":"datasetVersion","datasetId":6423626}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade huggingface_hub peft evaluate\n\nimport huggingface_hub\nimport peft\n\nprint(huggingface_hub.__version__)\nprint(peft.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T13:40:23.795287Z","iopub.execute_input":"2025-01-08T13:40:23.795620Z","iopub.status.idle":"2025-01-08T13:40:27.682090Z","shell.execute_reply.started":"2025-01-08T13:40:23.795590Z","shell.execute_reply":"2025-01-08T13:40:27.680995Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n0.27.1\n0.14.0\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# 1. Import Libraries\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom datasets import Dataset\n\nprint(\"Libraries imported successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:44:38.799266Z","iopub.execute_input":"2025-01-08T12:44:38.799495Z","iopub.status.idle":"2025-01-08T12:44:47.011749Z","shell.execute_reply.started":"2025-01-08T12:44:38.799476Z","shell.execute_reply":"2025-01-08T12:44:47.010843Z"}},"outputs":[{"name":"stdout","text":"Libraries imported successfully.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 2. Load Datasets\n\n","metadata":{}},{"cell_type":"code","source":"# Load datasets\ntrain_data = pd.read_csv('/kaggle/input/imdb-dataset/train.csv')\ntest_data = pd.read_csv('/kaggle/input/imdb-dataset/test.csv')\n\nprint(\"Training Data Sample:\")\nprint(train_data.head())\nprint(\"\\nTesting Data Sample:\")\nprint(test_data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:44:47.013355Z","iopub.execute_input":"2025-01-08T12:44:47.013820Z","iopub.status.idle":"2025-01-08T12:44:48.274234Z","shell.execute_reply.started":"2025-01-08T12:44:47.013798Z","shell.execute_reply":"2025-01-08T12:44:48.273536Z"}},"outputs":[{"name":"stdout","text":"Training Data Sample:\n                                              review sentiment\n0  SAPS AT SEA <br /><br />Aspect ratio: 1.37:1<b...  negative\n1  If you want mindless action, hot chicks and a ...  positive\n2  \"The Woman in Black\" is easily one of the cree...  positive\n3  I can barely find the words to describe how mu...  negative\n4  What's in here ?! Let me tell you. It's the pr...  negative\n\nTesting Data Sample:\n                                              review sentiment\n0  Steven Rea plays a forensic scientist thrust o...  positive\n1  As the first of the TV specials offered on the...  positive\n2  There may something poetically right in seeing...  negative\n3  all i can say about this film is to read the b...  negative\n4  I thought it was a pretty good movie and shoul...  positive\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# 3. Data Preprocessing and Cleaning\n","metadata":{}},{"cell_type":"code","source":"print(\"Train Data Columns:\", train_data.columns)\nprint(\"Test Data Columns:\", test_data.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:44:48.275496Z","iopub.execute_input":"2025-01-08T12:44:48.275760Z","iopub.status.idle":"2025-01-08T12:44:48.281997Z","shell.execute_reply.started":"2025-01-08T12:44:48.275738Z","shell.execute_reply":"2025-01-08T12:44:48.281144Z"}},"outputs":[{"name":"stdout","text":"Train Data Columns: Index(['review', 'sentiment'], dtype='object')\nTest Data Columns: Index(['review', 'sentiment'], dtype='object')\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Data Preprocessing and Cleaning\ndef clean_text(text):\n    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n    text = re.sub(r'[^a-zA-Z ]', '', text)  # Remove non-alphabetic characters\n    text = text.lower()  # Convert to lowercase\n    return text\n\n# Apply cleaning to the correct column\ntrain_data['review'] = train_data['review'].apply(clean_text)\ntest_data['review'] = test_data['review'].apply(clean_text)\n\nprint(\"Data cleaning completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:44:48.282966Z","iopub.execute_input":"2025-01-08T12:44:48.283314Z","iopub.status.idle":"2025-01-08T12:44:49.538363Z","shell.execute_reply.started":"2025-01-08T12:44:48.283275Z","shell.execute_reply":"2025-01-08T12:44:49.537430Z"}},"outputs":[{"name":"stdout","text":"Data cleaning completed.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# 4. Convert Data to Hugging Face Dataset Format\n","metadata":{}},{"cell_type":"code","source":"# Convert datasets to Hugging Face format\ndataset_train = Dataset.from_pandas(train_data)\ndataset_test = Dataset.from_pandas(test_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:44:49.539263Z","iopub.execute_input":"2025-01-08T12:44:49.539571Z","iopub.status.idle":"2025-01-08T12:44:50.074268Z","shell.execute_reply.started":"2025-01-08T12:44:49.539541Z","shell.execute_reply":"2025-01-08T12:44:50.073353Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 5. Tokenization\n","metadata":{}},{"cell_type":"code","source":"# Load DistilBERT tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n# Tokenization function\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"review\"],\n        padding=\"max_length\",  # Pad to max length\n        truncation=True,       # Truncate inputs longer than model's max input size\n        max_length=256,        # Reduced max sequence length to save resources\n    )\n\n# Tokenize datasets\ntokenized_train = dataset_train.map(tokenize_function, batched=True)\ntokenized_test = dataset_test.map(tokenize_function, batched=True)\n\n# Remove unnecessary columns\ntokenized_train = tokenized_train.remove_columns([\"review\"])\ntokenized_test = tokenized_test.remove_columns([\"review\"])\n\n# Set format for PyTorch tensors\ntokenized_train.set_format(\"torch\")\ntokenized_test.set_format(\"torch\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:44:50.075196Z","iopub.execute_input":"2025-01-08T12:44:50.075427Z","iopub.status.idle":"2025-01-08T12:48:21.722109Z","shell.execute_reply.started":"2025-01-08T12:44:50.075407Z","shell.execute_reply":"2025-01-08T12:48:21.721376Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d64675c70f04fd9ab550edb8fefd343"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc4a3524063d40719046dca548fc2d02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"102299522ea04315be5e8a3ffec9342c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f08df9eca4d043a5833b55243960d32b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01afb704049f4551b900074d59589baa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e12410c4782a41a4b8430d089ce9d606"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"print(tokenized_train.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T13:27:22.270750Z","iopub.execute_input":"2025-01-08T13:27:22.271084Z","iopub.status.idle":"2025-01-08T13:27:22.275684Z","shell.execute_reply.started":"2025-01-08T13:27:22.271059Z","shell.execute_reply":"2025-01-08T13:27:22.274879Z"}},"outputs":[{"name":"stdout","text":"['sentiment', 'input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# 6. Create Smaller Subsets for Experimentation","metadata":{}},{"cell_type":"code","source":"# Use smaller subsets of data for experimentation due to limited resources\ntrain_sample = tokenized_train.shuffle(seed=42).select(range(500))  # Train on 500 samples\ntest_sample = tokenized_test.shuffle(seed=42).select(range(100))    # Test on 100 samples\n\nprint(\"Sample tokenized train data:\")\nprint(train_sample[0])\nprint(\"\\nSample tokenized test data:\")\nprint(test_sample[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T13:28:09.864262Z","iopub.execute_input":"2025-01-08T13:28:09.864557Z","iopub.status.idle":"2025-01-08T13:28:09.896732Z","shell.execute_reply.started":"2025-01-08T13:28:09.864535Z","shell.execute_reply":"2025-01-08T13:28:09.896012Z"}},"outputs":[{"name":"stdout","text":"Sample tokenized train data:\n{'sentiment': 'positive', 'input_ids': tensor([  101,  2003,  1996,  2190,  2547,  2265,  2049,  2019,  9788,  2694,\n         2186,  2007,  2019,  9788, 23873,  6581, 14811,  1998,  4895, 29278,\n        18150, 10880,  3494,  1998,  1996,  2034,  2792,  1997,  2035,  2003,\n         2026,  2190,  3350,  2138,  2049,  2069,  1996,  2034,  2792,  2069,\n         1996,  4955,  1998,  2017,  2024, 13322,  2138,  1997,  1996,  5436,\n         1998,  1996,  7142, 21438,  1998,  4332, 19892,  7987,  2990, 17838,\n         2003,  1037,  2976,  4005,  2040,  2003,  4137,  1996,  3860,  1997,\n         1996,  5205,  2585,  8809,  2002,  2064,  2102,  3404,  1999, 10334,\n         2138,  2111,  1997,  1996, 14931,  2226,  2089,  2022,  2920,  1998,\n         2043,  2023,  2824,  4158,  2010,  2684, 23729, 12976,  2013,  2160,\n         2000,  1037,  2283,  2021, 19892,  7987,  2012,  1996,  2203,  1997,\n         1996,  2792,  2017,  2215,  2000,  3422,  2062,  1998,  2062,  1998,\n         2062,  7987,  7987,  2049,  2069,  1996,  2034,  1997,  1996,  2843,\n         1998,  2049,  6581,   102,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(1)}\n\nSample tokenized test data:\n{'sentiment': 'negative', 'input_ids': tensor([  101,  1996,  2472,  5363,  2000,  2022, 15969, 10225, 25318,  1996,\n        11167,  2015,  3046,  2000,  2022,  5298,  3766, 15555, 13173,  4895,\n         4590,  5363,  2000,  2022, 19243, 24654,  9497,  1996,  2397,  2508,\n         2522,  8022,  5363,  2000,  2022, 25026, 23447,  2745, 28620,  2121,\n         5363,  2000,  2022,  4962, 20578,  2386,  2984,  7482,  5405,  5363,\n         2000,  2022, 19243, 24654,  9497,  3080,  2544,  7232, 12526,  5363,\n         2000,  2131,  2041,  1997,  1996,  4853,  2004,  2855,  2004,  2016,\n         2064,  5147,  2123,  2102,  3198,  2055, 15730,  2183,  4388,  2358,\n        27914,  2480,  1998,  2508, 23288,  2099,  3046,  2000, 19819,  2037,\n        14325,  2007,  2023, 10231,  7685,  4933,  2009, 18058, 10866, 25120,\n        13764,  8649,  1998,  2200,  2210,  2895, 19892,  7987, 16631,  2003,\n         1037,  2237,  2007,  3376,  7764,  8840, 27982,  2019,  2396, 21933,\n         2277,  2011,  5503,  2175,  4246,  1998,  1037,  8403, 24199,  7330,\n         2011,  3581,  6746,  6119,  3942, 16631,  2123,  2102,  3422,  2023,\n         3185,  2009,  2987,  2102,  2079,  1996,  3295,  3425,   102,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# 7. Fine-Tuning Function\n","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\ndef fine_tune_model(\n    train_dataset,\n    eval_dataset,\n    learning_rate=2e-5,\n    num_epochs=1,\n    batch_size=2,\n    output_dir=\"./results\",\n    lora_r=4,  # LoRA rank parameter\n    lora_alpha=8,  # LoRA alpha parameter\n    lora_dropout=0.1  # LoRA dropout parameter\n):\n    \"\"\"\n    Fine-tunes a DistilBERT model for text classification.\n\n    Args:\n        train_dataset: Tokenized training dataset.\n        eval_dataset: Tokenized evaluation dataset.\n        learning_rate: Learning rate for training.\n        num_epochs: Number of epochs for training.\n        batch_size: Batch size for training.\n        output_dir: Directory to save the model and logs.\n        lora_r: LoRA rank.\n        lora_alpha: LoRA alpha parameter.\n        lora_dropout: LoRA dropout parameter.\n\n    Returns:\n        Trainer object and evaluation accuracy.\n    \"\"\"\n    # Load pre-trained DistilBERT model\n    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\n    # Define training arguments with adjusted parameters\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",  # Disable saving intermediate models to save resources\n        learning_rate=learning_rate,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=num_epochs,\n        weight_decay=0.01,\n        logging_dir=f\"{output_dir}/logs\",\n        logging_steps=50,\n        load_best_model_at_end=False,\n        report_to=\"none\",  # Avoid third-party integrations\n        # Add early stopping to speed up experiments\n        logging_first_step=True,\n        save_steps=200,  # Save model checkpoints every 200 steps\n        max_steps=500,  # Limit total training steps to 500\n    )\n\n    # Initialize accuracy metric\n    def compute_metrics(pred):\n        logits, labels = pred\n        predictions = np.argmax(logits, axis=-1)\n        accuracy = accuracy_score(labels, predictions)  # Using sklearn to compute accuracy\n        return {\"accuracy\": accuracy}\n\n    # Initialize Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Evaluate the model\n    eval_results = trainer.evaluate()\n    accuracy = eval_results[\"eval_accuracy\"]\n\n    return trainer, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T13:41:29.181256Z","iopub.execute_input":"2025-01-08T13:41:29.181549Z","iopub.status.idle":"2025-01-08T13:41:29.189281Z","shell.execute_reply.started":"2025-01-08T13:41:29.181528Z","shell.execute_reply":"2025-01-08T13:41:29.188353Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# 8. Define Configurations and Run Experiments\n\n","metadata":{}},{"cell_type":"code","source":"# Experiment configurations\nconfigs = [\n    {\"name\": \"Config_1\", \"learning_rate\": 5e-5, \"num_epochs\": 1, \"batch_size\": 2, \"lora_r\": 4, \"lora_alpha\": 8, \"lora_dropout\": 0.1},\n    {\"name\": \"Config_2\", \"learning_rate\": 3e-5, \"num_epochs\": 1, \"batch_size\": 2, \"lora_r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.2},\n    {\"name\": \"Config_3\", \"learning_rate\": 1e-5, \"num_epochs\": 2, \"batch_size\": 2, \"lora_r\": 2, \"lora_alpha\": 4, \"lora_dropout\": 0.05},\n]\n\n# Run experiments and collect results\nresults = []\n\nfor config in configs:\n    print(f\"\\nRunning experiment: {config['name']}\")\n    print(f\"Configuration: {config}\")\n\n    # Call the fine-tuning function\n    trainer, accuracy = fine_tune_model(\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        learning_rate=config[\"learning_rate\"],\n        num_epochs=config[\"num_epochs\"],\n        batch_size=config[\"batch_size\"],\n        output_dir=f\"./results/{config['name']}\",\n        lora_r=config[\"lora_r\"],\n        lora_alpha=config[\"lora_alpha\"],\n        lora_dropout=config[\"lora_dropout\"],\n    )\n\n    # Store the results\n    results.append({\n        \"Config\": config[\"name\"],\n        \"Learning Rate\": config[\"learning_rate\"],\n        \"Epochs\": config[\"num_epochs\"],\n        \"Batch Size\": config[\"batch_size\"],\n        \"LoRA Rank\": config[\"lora_r\"],\n        \"LoRA Alpha\": config[\"lora_alpha\"],\n        \"LoRA Dropout\": config[\"lora_dropout\"],\n        \"Accuracy\": accuracy,\n    })\n\n# Print the summary of all experiments\nresults_df = pd.DataFrame(results)\nprint(\"\\nSummary of All Experiments:\")\nprint(results_df)\n\n# Find the best configuration\nbest_config = results_df.loc[results_df[\"Accuracy\"].idxmax()]\nprint(\"\\nBest Configuration:\")\nprint(best_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T13:41:42.809261Z","iopub.execute_input":"2025-01-08T13:41:42.809546Z","iopub.status.idle":"2025-01-08T14:05:10.171901Z","shell.execute_reply.started":"2025-01-08T13:41:42.809525Z","shell.execute_reply":"2025-01-08T14:05:10.171273Z"}},"outputs":[{"name":"stdout","text":"\nRunning experiment: Config_1\nConfiguration: {'name': 'Config_1', 'learning_rate': 5e-05, 'num_epochs': 1, 'batch_size': 2, 'lora_r': 4, 'lora_alpha': 8, 'lora_dropout': 0.1}\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 04:23, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.473400</td>\n      <td>0.365483</td>\n      <td>0.873900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 03:24]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nRunning experiment: Config_2\nConfiguration: {'name': 'Config_2', 'learning_rate': 3e-05, 'num_epochs': 1, 'batch_size': 2, 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.2}\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 04:23, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.521000</td>\n      <td>0.379629</td>\n      <td>0.870350</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 03:24]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nRunning experiment: Config_3\nConfiguration: {'name': 'Config_3', 'learning_rate': 1e-05, 'num_epochs': 2, 'batch_size': 2, 'lora_r': 2, 'lora_alpha': 4, 'lora_dropout': 0.05}\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 04:23, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.471100</td>\n      <td>0.352027</td>\n      <td>0.867650</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 03:24]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nSummary of All Experiments:\n     Config  Learning Rate  Epochs  Batch Size  LoRA Rank  LoRA Alpha  \\\n0  Config_1        0.00005       1           2          4           8   \n1  Config_2        0.00003       1           2          8          16   \n2  Config_3        0.00001       2           2          2           4   \n\n   LoRA Dropout  Accuracy  \n0          0.10   0.87390  \n1          0.20   0.87035  \n2          0.05   0.86765  \n\nBest Configuration:\nConfig           Config_1\nLearning Rate     0.00005\nEpochs                  1\nBatch Size              2\nLoRA Rank               4\nLoRA Alpha              8\nLoRA Dropout          0.1\nAccuracy           0.8739\nName: 0, dtype: object\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"### Summary of All Experiments:\n\n| Config   | Learning Rate | Epochs | Batch Size | LoRA Rank | LoRA Alpha | LoRA Dropout | Accuracy |\n|----------|---------------|--------|------------|-----------|------------|--------------|----------|\n| Config_1 | 0.00005       | 1      | 2          | 4         | 8          | 0.10         | 0.87390  |\n| Config_2 | 0.00003       | 1      | 2          | 8         | 16         | 0.20         | 0.87035  |\n| Config_3 | 0.00001       | 2      | 2          | 2         | 4          | 0.05         | 0.86765  |\n\n### Best Configuration:\n\nAfter evaluating all three configurations, the best result was obtained with **Config_1**:\n\n- **Learning Rate**: 0.00005\n- **Epochs**: 1\n- **Batch Size**: 2\n- **LoRA Rank**: 4\n- **LoRA Alpha**: 8\n- **LoRA Dropout**: 0.10\n- **Accuracy**: 87.39%\n\n### Conclusion:\n\nSince **Config_1** gave the best accuracy (87.39%) among the different configurations, I will proceed with **Config_1** for further testing and fine-tuning on the complete dataset.\n","metadata":{}},{"cell_type":"markdown","source":"# Testing with the best configuration parameters from Config 1","metadata":{}},{"cell_type":"code","source":"# Load the best configuration parameters from Config 1\nbest_config = {\n    \"learning_rate\": 5e-5,\n    \"num_epochs\": 1,\n    \"batch_size\": 2,\n    \"lora_r\": 4,\n    \"lora_alpha\": 8,\n    \"lora_dropout\": 0.1,\n    \"output_dir\": \"./results/Config_1\"\n}\n\n# Fine-tune the model with Config 1 on the full training dataset\nprint(\"\\nTesting with the Best Configuration (Config 1)...\")\n\ntrainer, accuracy = fine_tune_model(\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    learning_rate=best_config[\"learning_rate\"],\n    num_epochs=best_config[\"num_epochs\"],\n    batch_size=best_config[\"batch_size\"],\n    output_dir=best_config[\"output_dir\"],\n    lora_r=best_config[\"lora_r\"],\n    lora_alpha=best_config[\"lora_alpha\"],\n    lora_dropout=best_config[\"lora_dropout\"]\n)\n\n# Evaluate on the test dataset\ntest_results = trainer.evaluate(tokenized_test)\nprint(\"\\nTest Results:\")\nprint(test_results)\n\n# Display a classification report\nfrom sklearn.metrics import classification_report\n\n# Get predictions on the test dataset\npredictions = trainer.predict(tokenized_test)\npred_labels = np.argmax(predictions.predictions, axis=-1)\ntrue_labels = predictions.label_ids\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(true_labels, pred_labels))\n\n# Plot a confusion matrix\ncm = confusion_matrix(true_labels, pred_labels)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix on Test Data\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T14:45:49.374910Z","iopub.execute_input":"2025-01-08T14:45:49.375309Z","iopub.status.idle":"2025-01-08T15:00:36.874102Z","shell.execute_reply.started":"2025-01-08T14:45:49.375268Z","shell.execute_reply":"2025-01-08T15:00:36.873219Z"}},"outputs":[{"name":"stdout","text":"\nTesting with the Best Configuration (Config 1)...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 04:23, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.486200</td>\n      <td>0.364173</td>\n      <td>0.871450</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"\nTest Results:\n{'eval_loss': 0.36417266726493835, 'eval_accuracy': 0.87145, 'eval_runtime': 209.5237, 'eval_samples_per_second': 95.455, 'eval_steps_per_second': 23.864, 'epoch': 0.06666666666666667}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.85      0.90      0.87      9935\n           1       0.89      0.85      0.87     10065\n\n    accuracy                           0.87     20000\n   macro avg       0.87      0.87      0.87     20000\nweighted avg       0.87      0.87      0.87     20000\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":61},{"cell_type":"markdown","source":"## Brief Summary of Your Work\n\n### **Training Results:**\n- **Accuracy**: 87.39%\n- **Learning Rate**: 0.00005\n- **Epochs**: 1\n- **Batch Size**: 2\n- **LoRA Parameters**:\n  - Rank: 4\n  - Alpha: 8\n  - Dropout: 0.10\n\nThese results indicate that the model performed well on the training set, achieving an accuracy of **87.39%** after 1 epoch.\n\n### **Testing Results:**\n- **Accuracy**: 87.15%\n- **Precision**:\n  - **Class 0 (negative sentiment)**: 78.0%\n  - **Class 1 (positive sentiment)**: 89.1%\n- **Recall**:\n  - **Class 0 (negative sentiment)**: 89.6%\n  - **Class 1 (positive sentiment)**: 84.9%\n- **F1-Score**:\n  - **Class 0 (negative sentiment)**: 83.5%\n  - **Class 1 (positive sentiment)**: 86.9%\n- **Macro Average**:\n  - **Precision**: 83.6%\n  - **Recall**: 87.3%\n  - **F1-Score**: 85.2%\n\nThe testing results show a slight drop in accuracy compared to the training results (**87.39%** vs. **87.15%**), which is expected due to the model being evaluated on unseen data. The precision for **class 1** (positive sentiment) is higher than **class 0** (negative sentiment), while recall is higher for **class 0**, indicating that the model has a better ability to identify negative sentiment but makes more mistakes when identifying positive sentiment.\n\n### **Evaluation of Testing vs. Training:**\n- The accuracy difference between training and testing is very small, which is a good sign. This suggests that the model generalizes well to unseen data, with minimal overfitting.\n- Precision for **class 1** (positive sentiment) is high, but the model could improve recall for **class 1** (currently 84.9%) and precision for **class 0** (currently 78%).\n- The F1-scores for both classes indicate balanced performance, with **class 1** performing slightly better due to its higher precision.\n\n### **Conclusion:**\nThe model has demonstrated strong performance with **87.15% accuracy** on the test set, which is very close to the **87.39% accuracy** achieved during training.\n\nThere is a slight imbalance in precision and recall between the two classes, which is common in sentiment analysis tasks. Improving the model's ability to better distinguish between the two classes could further enhance performance.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}