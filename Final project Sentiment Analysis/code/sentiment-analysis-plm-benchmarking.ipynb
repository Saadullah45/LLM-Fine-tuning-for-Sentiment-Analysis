{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10370500,"sourceType":"datasetVersion","datasetId":6423626}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade huggingface_hub peft evaluate\n\nimport huggingface_hub\nimport peft\n\nprint(huggingface_hub.__version__)\nprint(peft.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:17:38.819829Z","iopub.execute_input":"2025-01-08T17:17:38.820086Z","iopub.status.idle":"2025-01-08T17:17:50.435397Z","shell.execute_reply.started":"2025-01-08T17:17:38.820056Z","shell.execute_reply":"2025-01-08T17:17:50.434619Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\nCollecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface_hub, peft, evaluate\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.24.7\n    Uninstalling huggingface-hub-0.24.7:\n      Successfully uninstalled huggingface-hub-0.24.7\nSuccessfully installed evaluate-0.4.3 huggingface_hub-0.27.1 peft-0.14.0\n0.27.1\n0.14.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import wandb library\nimport wandb\n\n# Log in to wandb using your API key\nwandb.login(key='5656979c25c03784238533b8e79512ab61bf94e2')\n\nprint(\"Logged in to wandb successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:41:52.996590Z","iopub.execute_input":"2025-01-08T17:41:52.996957Z","iopub.status.idle":"2025-01-08T17:41:53.102080Z","shell.execute_reply.started":"2025-01-08T17:41:52.996929Z","shell.execute_reply":"2025-01-08T17:41:53.101364Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhadiabbas223\u001b[0m (\u001b[33mhadiabbas223-iba-institute-of-business-administration\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Logged in to wandb successfully.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# 1. Import Libraries\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom datasets import Dataset\n\nprint(\"Libraries imported successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:17:50.436379Z","iopub.execute_input":"2025-01-08T17:17:50.436865Z","iopub.status.idle":"2025-01-08T17:18:00.460353Z","shell.execute_reply.started":"2025-01-08T17:17:50.436816Z","shell.execute_reply":"2025-01-08T17:18:00.459454Z"}},"outputs":[{"name":"stdout","text":"Libraries imported successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# 2. Load Datasets\n\n","metadata":{}},{"cell_type":"code","source":"# Load datasets\ntrain_data = pd.read_csv('/kaggle/input/imdb-dataset/train.csv')\ntest_data = pd.read_csv('/kaggle/input/imdb-dataset/test.csv')\n\nprint(\"Training Data Sample:\")\nprint(train_data.head())\nprint(\"\\nTesting Data Sample:\")\nprint(test_data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:18:00.461226Z","iopub.execute_input":"2025-01-08T17:18:00.461866Z","iopub.status.idle":"2025-01-08T17:18:02.282621Z","shell.execute_reply.started":"2025-01-08T17:18:00.461841Z","shell.execute_reply":"2025-01-08T17:18:02.281539Z"}},"outputs":[{"name":"stdout","text":"Training Data Sample:\n                                              review sentiment\n0  SAPS AT SEA <br /><br />Aspect ratio: 1.37:1<b...  negative\n1  If you want mindless action, hot chicks and a ...  positive\n2  \"The Woman in Black\" is easily one of the cree...  positive\n3  I can barely find the words to describe how mu...  negative\n4  What's in here ?! Let me tell you. It's the pr...  negative\n\nTesting Data Sample:\n                                              review sentiment\n0  Steven Rea plays a forensic scientist thrust o...  positive\n1  As the first of the TV specials offered on the...  positive\n2  There may something poetically right in seeing...  negative\n3  all i can say about this film is to read the b...  negative\n4  I thought it was a pretty good movie and shoul...  positive\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\n\n# Load your training and test data\ntrain_data = pd.read_csv('/kaggle/input/imdb-dataset/train.csv')\ntest_data = pd.read_csv('/kaggle/input/imdb-dataset/test.csv')\n\n# Preprocessing: Convert the sentiment into binary labels (positive=1, negative=0)\ntrain_data['label'] = train_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\ntest_data['label'] = test_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n\n# Use only 10% of the data for quick training and testing\ntrain_data_sample = train_data.sample(frac=0.1, random_state=42).reset_index(drop=True)\ntest_data_sample = test_data.sample(frac=0.1, random_state=42).reset_index(drop=True)\n\n# Tokenization\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Define the dataset for Hugging Face Trainer\ntrain_dataset = Dataset.from_pandas(train_data_sample[['review', 'label']])\ntest_dataset = Dataset.from_pandas(test_data_sample[['review', 'label']])\n\n# Tokenize the reviews\ndef tokenize_function(examples):\n    return tokenizer(examples['review'], padding='max_length', truncation=True)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:44:56.329976Z","iopub.execute_input":"2025-01-08T17:44:56.330326Z","iopub.status.idle":"2025-01-08T17:45:21.770562Z","shell.execute_reply.started":"2025-01-08T17:44:56.330302Z","shell.execute_reply":"2025-01-08T17:45:21.769886Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bfdb5c587db4c7ab592c39a0e3f13c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aee96fe0da742d1a15791ed2eba54b0"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Define model\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english', num_labels=2)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=3,              # number of training epochs\n    per_device_train_batch_size=8,   # batch size for training\n    per_device_eval_batch_size=8,    # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",     # evaluation is done at the end of each epoch\n    save_strategy=\"epoch\",           # save model at the end of each epoch\n    disable_tqdm=False,\n    load_best_model_at_end=True,     # load best model after training\n)\n\n# Trainer setup\ntrainer = Trainer(\n    model=model,                         # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=test_dataset,           # evaluation dataset\n    compute_metrics=None,                # Leave metrics calculation separate\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate the model\nresults = trainer.evaluate()\n\nprint(f\"Evaluation Results: {results}\")\n\n# Predictions for classification report\npredictions = trainer.predict(test_dataset)\ny_pred = predictions.predictions.argmax(axis=-1)\n\n# Print classification report\nprint(classification_report(test_data_sample['label'], y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:45:31.516175Z","iopub.execute_input":"2025-01-08T17:45:31.516465Z","iopub.status.idle":"2025-01-08T17:51:36.924671Z","shell.execute_reply.started":"2025-01-08T17:45:31.516442Z","shell.execute_reply":"2025-01-08T17:51:36.923911Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [564/564 05:23, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.346500</td>\n      <td>0.256328</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.303100</td>\n      <td>0.304224</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.106200</td>\n      <td>0.366728</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.25632813572883606, 'eval_runtime': 20.0697, 'eval_samples_per_second': 99.653, 'eval_steps_per_second': 6.228, 'epoch': 3.0}\n              precision    recall  f1-score   support\n\n           0       0.93      0.86      0.89       966\n           1       0.88      0.94      0.91      1034\n\n    accuracy                           0.90      2000\n   macro avg       0.90      0.90      0.90      2000\nweighted avg       0.90      0.90      0.90      2000\n\n","output_type":"stream"}],"execution_count":7}]}